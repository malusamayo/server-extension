{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario\n",
    "Alice is competing in Kaggle's Titanic competitions. Following the advice from an internet article, she tried heavy feature engineering and a simple classifying model. However, she found that the model did not perform as well as what the original article claimed. She wondered what went wrong, but couldn't find the problem despite much effort. **Could you help her find problems in her notebook and improve the model's performance?**\n",
    "\n",
    "### Instruction\n",
    "+ You are free to use the Internet to search for any problems you might encounter.\n",
    "+ You overall goal is to improve the model's performance, but you might prioritize finding the underlying problems in the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's define a print function that asserts whether or not a feature has been processed. \n",
    "def status(feature):\n",
    "    print('Processing', feature, ': ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loading the data\n",
    "\n",
    "One trick when starting a machine learning problem is to append the training set to the test set together.\n",
    "\n",
    "We'll engineer new features using the train set to prevent information leakage. Then we'll add these variables to the test set.\n",
    "\n",
    "Let's load the train and test sets and append them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading train data\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "train_len = len(train)\n",
    "\n",
    "# reading test data\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# extracting and then removing the targets from the training data \n",
    "targets = train.Survived\n",
    "train.drop(['Survived'], 1, inplace=True)\n",
    "\n",
    "\n",
    "# merging train data and test data for future feature engineering\n",
    "# we'll also remove the PassengerID since this is not an informative feature\n",
    "combined = train.append(test)\n",
    "combined.reset_index(inplace=True)\n",
    "combined.drop(['index', 'PassengerId'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Title\n",
    "This function parses the names and extract the titles. Then, it maps the titles to categories of titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = set()\n",
    "for name in combined['Name']:\n",
    "    titles.add(name.split(',')[1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title_Dictionary = {\n",
    "    \"Capt\": \"Officer\",\n",
    "    \"Col\": \"Officer\",\n",
    "    \"Major\": \"Officer\",\n",
    "    \"Jonkheer\": \"Royalty\",\n",
    "    \"Don\": \"Royalty\",\n",
    "    \"Sir\" : \"Royalty\",\n",
    "    \"Dr\": \"Officer\",\n",
    "    \"Rev\": \"Officer\",\n",
    "    \"the Countess\":\"Royalty\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Ms\": \"Mrs\",\n",
    "    \"Mr\" : \"Mr\",\n",
    "    \"Mrs\" : \"Mrs\",\n",
    "    \"Miss\" : \"Miss\",\n",
    "    \"Master\" : \"Master\",\n",
    "    \"Lady\" : \"Royalty\"\n",
    "}\n",
    "\n",
    "# we extract the title from each name\n",
    "combined['Title'] = combined['Name'].map(lambda name:name.split('.')[0].strip())\n",
    "\n",
    "# a map of more aggregated title\n",
    "# we map each title\n",
    "combined['Title'] = combined.Title.map(Title_Dictionary)\n",
    "status('Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some names not in dict are mapped to \"Royalty\"\n",
    "combined[combined['Title'].isnull()]\n",
    "combined.at[combined['Title'].isnull(), \"Title\"] = \"Royalty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Ages\n",
    "Let's create a function that fills in the missing age in combined based on these different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_train = combined.iloc[:train_len].groupby(['Sex','Pclass','Title'])\n",
    "grouped_median_train = grouped_train.median()\n",
    "grouped_median_train = grouped_median_train.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_age(row):\n",
    "    condition = (\n",
    "        (grouped_median_train['Sex'] == row['Sex']) & \n",
    "        (grouped_median_train['Title'] == row['Title']) & \n",
    "        (grouped_median_train['Pclass'] == row['Pclass'])\n",
    "    ) \n",
    "    return grouped_median_train[condition]['Age'].values[0]\n",
    "\n",
    "combined['Age'] = combined.apply(lambda row: fill_age(row), axis=1)\n",
    "status('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now process the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we clean the Name variable\n",
    "combined.drop('Name', axis=1, inplace=True)\n",
    "\n",
    "# encoding in dummy variable\n",
    "titles_dummies = pd.get_dummies(combined['Title'], prefix='Title')\n",
    "combined = pd.concat([combined, titles_dummies], axis=1)\n",
    "\n",
    "# removing the title variable\n",
    "combined.drop('Title', axis=1, inplace=True)\n",
    "\n",
    "status('names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's one missing fare value - replacing it with the mean.\n",
    "combined.Fare.fillna(combined.iloc[:train_len].Fare.mean(), inplace=True)\n",
    "status('fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two missing embarked values - filling them with the most frequent one in the train  set(S)\n",
    "combined.Embarked.fillna('S', inplace=True)\n",
    "# dummy encoding \n",
    "embarked_dummies = pd.get_dummies(combined['Embarked'], prefix='Embarked')\n",
    "combined = pd.concat([combined, embarked_dummies], axis=1)\n",
    "combined.drop('Embarked', axis=1, inplace=True)\n",
    "status('embarked')\n",
    "combined.Cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing missing cabins with U (for Uknown)\n",
    "combined.Cabin.fillna('U', inplace=True)\n",
    "\n",
    "# mapping each Cabin value with the cabin letter\n",
    "combined['Cabin'] = combined['Cabin'].map(lambda c: c[0])\n",
    "\n",
    "# dummy encoding ...\n",
    "cabin_dummies = pd.get_dummies(combined['Cabin'], prefix='Cabin')    \n",
    "combined = pd.concat([combined, cabin_dummies], axis=1)\n",
    "\n",
    "combined.drop('Cabin', axis=1, inplace=True)\n",
    "status('cabin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function replaces NaN values with U (for <i>Unknow</i>). It then maps each Cabin value to the first letter.\n",
    "Then it encodes the cabin values using dummy encoding again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping string values to numerical one \n",
    "combined['Sex'] = combined['Sex'].map({'male':1, 'female':0})\n",
    "status('Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function maps the string values male and female to 1 and 0 respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding into 3 categories:\n",
    "pclass_dummies = pd.get_dummies(combined['Pclass'], prefix=\"Pclass\")\n",
    "\n",
    "# adding dummy variable\n",
    "combined = pd.concat([combined, pclass_dummies],axis=1)\n",
    "\n",
    "# removing \"Pclass\"\n",
    "combined.drop('Pclass',axis=1,inplace=True)\n",
    "\n",
    "status('Pclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function encodes the values of Pclass (1,2,3) using a dummy encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n",
    "def cleanTicket(ticket):\n",
    "    ticket = ticket.replace('.','')\n",
    "    ticket = ticket.replace('/','')\n",
    "    ticket = ticket.split()\n",
    "    ticket = map(lambda t : t.strip(), ticket)\n",
    "    ticket = list(filter(lambda t : not t.isdigit(), ticket))\n",
    "    if len(ticket) > 0:\n",
    "        return ticket[0]\n",
    "    else: \n",
    "        return 'XXX'\n",
    "\n",
    "\n",
    "# Extracting dummy variables from tickets:\n",
    "\n",
    "combined['Ticket'] = combined['Ticket'].map(cleanTicket)\n",
    "tickets_dummies = pd.get_dummies(combined['Ticket'], prefix='Ticket')\n",
    "combined = pd.concat([combined, tickets_dummies], axis=1)\n",
    "combined.drop('Ticket', inplace=True, axis=1)\n",
    "\n",
    "status('Ticket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introducing a new feature : the size of families (including the passenger)\n",
    "combined['FamilySize'] = combined['Parch'] + combined['SibSp'] + 1\n",
    "\n",
    "# introducing other features based on the family size\n",
    "combined['Singleton'] = combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n",
    "combined['SmallFamily'] = combined['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n",
    "combined['LargeFamily'] = combined['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n",
    "\n",
    "status('family')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(clf, X, y, scoring='accuracy'):\n",
    "    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n",
    "    return np.mean(xval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recovering the train set and the test set from the combined dataset is an easy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pd.read_csv('./data/train.csv', usecols=['Survived'])['Survived'].values\n",
    "train = combined.iloc[:train_len]\n",
    "test = combined.iloc[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, max_features='sqrt')\n",
    "clf = clf.fit(train, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['feature'] = train.columns\n",
    "features['importance'] = clf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "features.set_index('feature', inplace=True)\n",
    "\n",
    "features.plot(kind='barh', figsize=(25, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(clf, prefit=True)\n",
    "train_reduced = model.transform(train)\n",
    "test_reduced = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "print('Cross-validation of : {0}'.format(model.__class__))\n",
    "score = compute_score(clf=model, X=train_reduced, y=targets, scoring='accuracy')\n",
    "print('CV score = {0}'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
